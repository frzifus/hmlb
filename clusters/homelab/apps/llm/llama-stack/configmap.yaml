---
kind: ConfigMap
apiVersion: v1
metadata:
  name: run-config
  namespace: llm
data:
data:
  config.yaml: |
    version: '2'
    image_name: remote-vllm
    apis:
    - inference
    - telemetry
    - tool_runtime
    providers:
      inference:
      - provider_id: vllm-inference
        provider_type: remote::vllm
        config:
          url: ${env.VLLM_URL:http://localhost:8000/v1}
          max_tokens: ${env.VLLM_MAX_TOKENS:1028}
          api_token: ${env.VLLM_API_TOKEN:fake}
          tls_verify: false
      telemetry:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          service_name: ${env.OTEL_SERVICE_NAME:}
          sinks: ${env.TELEMETRY_SINKS:console,sqlite}
          otel_trace_endpoint: ${env.CUSTOM_OTEL_TRACE_ENDPOINT:}
          sqlite_db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/trace_store.db
      tool_runtime:
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_SEARCH_API_KEY:}
          max_results: 3
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
    metadata_store:
      type: sqlite
      db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/registry.db
    models:
    - metadata: {}
      model_id: ${env.INFERENCE_MODEL}
      provider_id: vllm-inference
      model_type: llm
    shields: []
    vector_dbs: []
    datasets: []
    scoring_fns: []
    benchmarks: []
    tool_groups:
    - toolgroup_id: builtin::websearch
      provider_id: tavily-search
    - toolgroup_id: mcp::kubernetes
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: "http://mcp-k8s-server:8080/sse"
    - toolgroup_id: mcp::homeassistant
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: "http://mcp-proxy-hass.llm.svc.cluster.local:8080/sse"
    server:
      port: 8321
