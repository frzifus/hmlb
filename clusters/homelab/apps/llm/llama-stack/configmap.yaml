---
kind: ConfigMap
apiVersion: v1
metadata:
  name: run-config
  namespace: llm
data:
  config.yaml: |
    apis:
    - agents
    - batches
    - datasetio
    - eval
    - files
    - inference
    - post_training
    - safety
    - scoring
    - tool_runtime
    - vector_io
    image_name: starter
    providers:
      agents:
      - config:
          persistence:
            agent_state:
              backend: kv_default
              namespace: agents
            responses:
              backend: sql_default
              max_write_queue_size: 10000
              num_writers: 4
              table_name: responses
        provider_id: meta-reference
        provider_type: inline::meta-reference
      batches:
      - config:
          kvstore:
            backend: kv_default
            namespace: batches
        provider_id: reference
        provider_type: inline::reference
      datasetio:
      - config:
          kvstore:
            backend: kv_default
            namespace: datasetio::huggingface
        provider_id: huggingface
        provider_type: remote::huggingface
      - config:
          kvstore:
            backend: kv_default
            namespace: datasetio::localfs
        provider_id: localfs
        provider_type: inline::localfs
      eval:
      - config:
          kvstore:
            backend: kv_default
            namespace: eval
        provider_id: meta-reference
        provider_type: inline::meta-reference
      files:
      - config:
          metadata_store:
            backend: sql_default
            table_name: files_metadata
          storage_dir: /.llama/distributions/starter/files
        provider_id: meta-reference-files
        provider_type: inline::localfs
      inference:
      - config:
          api_key: 'fake'
          base_url: http://ollama:11434/v1
        provider_id: openai
        provider_type: remote::openai
      post_training:
      - config:
          checkpoint_format: meta
        provider_id: torchtune-cpu
        provider_type: inline::torchtune-cpu
      safety:
      - config:
          excluded_categories: []
        provider_id: llama-guard
        provider_type: inline::llama-guard
      - config: {}
        provider_id: code-scanner
        provider_type: inline::code-scanner
      scoring:
      - config: {}
        provider_id: basic
        provider_type: inline::basic
      - config: {}
        provider_id: llm-as-judge
        provider_type: inline::llm-as-judge
      - config:
          openai_api_key: '********'
        provider_id: braintrust
        provider_type: inline::braintrust
      tool_runtime:
      - config:
          api_key: ${env.TAVILY_SEARCH_API_KEY:}
          max_results: 3
        provider_id: tavily-search
        provider_type: remote::tavily-search
      - config: {}
        provider_id: rag-runtime
        provider_type: inline::rag-runtime
      - config: {}
        provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
      vector_io:
      - config:
          persistence:
            backend: kv_default
            namespace: vector_io::faiss
        provider_id: faiss
        provider_type: inline::faiss
      - config:
          db_path: /.llama/distributions/starter/sqlite_vec.db
          persistence:
            backend: kv_default
            namespace: vector_io::sqlite_vec
        provider_id: sqlite-vec
        provider_type: inline::sqlite-vec
    registered_resources:
      benchmarks: []
      datasets: []
      models: []
      scoring_fns: []
      shields: []
      tool_groups:
      - provider_id: tavily-search
        toolgroup_id: builtin::websearch
      - provider_id: rag-runtime
        toolgroup_id: builtin::rag
      - toolgroup_id: mcp::kubernetes
        provider_id: model-context-protocol
        mcp_endpoint:
          uri: "http://mcp-k8s-server:8080/sse"
      - toolgroup_id: mcp::homeassistant
        provider_id: model-context-protocol
        mcp_endpoint:
          uri: "http://mcp-proxy-hass.llm.svc.cluster.local:8080/sse"
      vector_stores: []
    server:
      port: 8321
      workers: 1
    storage:
      backends:
        kv_default:
          db_path: /.llama/distributions/starter/kvstore.db
          type: kv_sqlite
        sql_default:
          db_path: /.llama/distributions/starter/sql_store.db
          type: sql_sqlite
      stores:
        conversations:
          backend: sql_default
          table_name: openai_conversations
        inference:
          backend: sql_default
          max_write_queue_size: 10000
          num_writers: 4
          table_name: inference_store
        metadata:
          backend: kv_default
          namespace: registry
    telemetry:
      enabled: true
    version: 2
