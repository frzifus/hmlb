apiVersion: v1
data:
  agent.py: |
    #!/usr/bin/env python3

    # kubectl create configmap llamastack-agent --from-file=agent.py

    import os
    import logging
    import time
    from fastapi import FastAPI, HTTPException, Request, Header
    from pydantic import BaseModel, Field
    from typing import List, Optional, Literal, Dict, Any
    from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger
    from llama_stack_client.lib.agents.client_tool import client_tool

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    app = FastAPI(title="OpenAI-Compatible LlamaStack API", version="1.0")

    MODEL_ID = "vllm"
    AGENT_SESSION_ID = None
    AGENT_INSTANCE = None

    @client_tool
    def calculator(x: float, y: float, operation: str) -> dict:
        """
        Perform a basic arithmetic operation on two numbers.

        :param x: First number
        :param y: Second number
        :param operation: The operation to perform: 'add', 'subtract', 'multiply', or 'divide'
        :returns: A dictionary with keys 'success' and either 'result' or 'error'
        """
        print(f"Call calculator: {x} {operation}, {y}", file=sys.stdout, flush=True)
        try:
            if operation == "add":
                result = x + y
            elif operation == "subtract":
                result = x - y
            elif operation == "multiply":
                result = x * y
            elif operation == "divide":
                if y == 0:
                    return {"success": False, "error": "Cannot divide by zero"}
                result = x / y
            else:
                return {"success": False, "error": "Invalid operation"}

            return {"success": True, "result": result}
        except Exception as e:
            return {"success": False, "error": str(e)}

    class CompletionRequest(BaseModel):
        model: str
        prompt: str
        max_tokens: Optional[int] = 256
        temperature: Optional[float] = 1.0
        top_p: Optional[float] = 0.9
        stop: Optional[List[str]] = None


    class CompletionChoice(BaseModel):
        text: str
        index: int
        logprobs: Optional[Any] = None
        finish_reason: str


    class CompletionResponse(BaseModel):
        id: str
        object: Literal["text_completion"]
        created: int
        model: str
        choices: List[CompletionChoice]


    class ModelInfo(BaseModel):
        id: str
        object: Literal["model"] = "model"
        owned_by: str = "owner"


    class ModelList(BaseModel):
        object: Literal["list"]
        data: List[ModelInfo]

    def initialize_agent():
        global AGENT_INSTANCE, AGENT_SESSION_ID

        host = os.getenv("LLAMA_HOST", "localhost")
        port = int(os.getenv("LLAMA_PORT", 8080))
        client = LlamaStackClient(base_url=f"http://{host}:{port}")

        available_models = [
            model.identifier
            for model in client.models.list()
            if model.model_type == "llm" and "guard" not in model.identifier
        ]

        if MODEL_ID not in available_models:
            raise RuntimeError(f"Model `{MODEL_ID}` not found in {available_models}")

        agent = Agent(
            client,
            model=MODEL_ID,
            instructions="You are a helpful assistant. Use tools when necessary.",
            sampling_params={
                "strategy": {"type": "top_p", "temperature": 1.0, "top_p": 0.9},
            },
            #tools=[],
            tools=[calculator],
        )

        session_id = agent.create_session("openai-compatible-session")
        AGENT_INSTANCE = agent
        AGENT_SESSION_ID = session_id
        logger.info(f"Initialized agent with model {MODEL_ID} and session {session_id}")

    @app.on_event("startup")
    def on_startup():
        initialize_agent()

    @app.get("/v1/models", response_model=ModelList)
    def get_models():
        return ModelList(object="list", data=[ModelInfo(id=MODEL_ID)])

    @app.post("/v1/completions")
    def completions(request: CompletionRequest):
        if request.model != MODEL_ID:
            raise HTTPException(status_code=404, detail="Model not found")

        if not AGENT_INSTANCE or not AGENT_SESSION_ID:
            raise HTTPException(status_code=500, detail="Agent not initialized")

        messages = request.prompt if isinstance(request.prompt, list) else [request.prompt]

        response = AGENT_INSTANCE.create_turn(
            messages=[{"role": "user", "content": m} for m in messages],
            session_id=AGENT_SESSION_ID,
            stream=False,
        )

        print(f"Response from agent: {response}")
        content = ""

        try:
            if hasattr(response, "output_message"):
                content = response.output_message.content
            elif hasattr(response, "steps") and len(response.steps) > 0:
                step = response.steps[0]
                if hasattr(step, "api_model_response"):
                    content = step.api_model_response.content

            if not content:
                raise HTTPException(status_code=500, detail="No assistant response received")

        except Exception as e:
            print(f"Error while processing the log: {e}")
            raise HTTPException(status_code=500, detail="Error processing the log")

        if not content:
            raise HTTPException(status_code=500, detail="No assistant response received")

        return {
            "id": "cmpl-1234",
            "object": "text_completion",
            "created": int(time.time()),
            "model": MODEL_ID,
            "choices": [
                {
                    "text": content.strip(),
                    "index": 0,
                    "logprobs": None,
                    "finish_reason": "stop"
                }
            ]
        }
kind: ConfigMap
metadata:
  name: llamastack-agent
  namespace: llm
