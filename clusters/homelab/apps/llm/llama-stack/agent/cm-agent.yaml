apiVersion: v1
data:
  agent.py: |
    #!/usr/bin/env python3

    # kubectl create configmap llamastack-agent --from-file=agent.py

    import os
    import logging
    import time
    import httpx
    from fastapi import FastAPI, HTTPException, Request, Header
    from pydantic import BaseModel, Field
    from typing import List, Optional, Literal, Dict, Any
    from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger
    from llama_stack_client.lib.agents.client_tool import client_tool

    from opentelemetry import trace
    from opentelemetry.sdk.resources import Resource
    from opentelemetry.sdk.trace import TracerProvider
    from opentelemetry.sdk.trace.export import SimpleSpanProcessor
    from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

    from opentelemetry.propagate import inject
    from opentelemetry.context import get_current
    from opentelemetry.propagators.textmap import default_setter

    from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

    app = FastAPI()

    class OTelHttpxClient(httpx.Client):
        def send(self, request, *args, **kwargs):
            # Inject current trace context into the request headers
            inject(request.headers, context=get_current(), setter=default_setter)
            return super().send(request, *args, **kwargs)

    http_client = OTelHttpxClient()

    trace.set_tracer_provider(
        TracerProvider(
            resource=Resource.create({"service.name": "llamastack-agent"})
        )
    )

    otlp_trace_exporter = OTLPSpanExporter( # NOTE: Testing backend.
        endpoint="http://backend.observability.svc.cluster.local:4317",
        insecure=True
    )

    trace.get_tracer_provider().add_span_processor(
        SimpleSpanProcessor(otlp_trace_exporter)
    )

    tracer = trace.get_tracer(__name__)


    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    app = FastAPI(title="OpenAI-Compatible LlamaStack API", version="1.0")

    MODEL_ID = "vllm"

    # Mapping for user-facing model name to internal model identifier
    MODEL_ALIAS_MAP = {
        "agent": MODEL_ID  # "agent" is exposed, but internally it's always "vllm"
    }

    AGENT_SESSION_ID = None
    AGENT_INSTANCE = None

    @client_tool
    def calculator(x: float, y: float, operation: str) -> dict:
        """
        Perform a basic arithmetic operation on two numbers.

        :param x: First number
        :param y: Second number
        :param operation: The operation to perform: 'add', 'subtract', 'multiply', or 'divide'
        :returns: A dictionary with keys 'success' and either 'result' or 'error'
        """
        print(f"Call calculator: {x} {operation}, {y}", file=sys.stdout, flush=True)
        try:
            if operation == "add":
                result = x + y
            elif operation == "subtract":
                result = x - y
            elif operation == "multiply":
                result = x * y
            elif operation == "divide":
                if y == 0:
                    return {"success": False, "error": "Cannot divide by zero"}
                result = x / y
            else:
                return {"success": False, "error": "Invalid operation"}

            return {"success": True, "result": result}
        except Exception as e:
            return {"success": False, "error": str(e)}

    class CompletionRequest(BaseModel):
        model: str
        prompt: str
        max_tokens: Optional[int] = 256
        temperature: Optional[float] = 1.0
        top_p: Optional[float] = 0.9
        stop: Optional[List[str]] = None

    class CompletionChoice(BaseModel):
        text: str
        index: int
        logprobs: Optional[Any] = None
        finish_reason: str

    class CompletionResponse(BaseModel):
        id: str
        object: Literal["text_completion"]
        created: int
        model: str
        choices: List[CompletionChoice]

    class ChatMessage(BaseModel):
        role: Literal["system", "user", "assistant"]
        content: str

    class ChatCompletionRequest(BaseModel):
        model: str
        messages: List[ChatMessage]
        temperature: Optional[float] = 1.0
        top_p: Optional[float] = 0.9
        max_tokens: Optional[int] = 256

    class ChatCompletionChoice(BaseModel):
        index: int
        message: ChatMessage
        finish_reason: str

    class ChatCompletionResponse(BaseModel):
        id: str
        object: Literal["chat.completion"]
        created: int
        model: str
        choices: List[ChatCompletionChoice]

    class ModelInfo(BaseModel):
        id: str
        object: Literal["model"] = "model"
        owned_by: str = "owner"

    class ModelList(BaseModel):
        object: Literal["list"]
        data: List[ModelInfo]

    def initialize_agent():
        global AGENT_INSTANCE, AGENT_SESSION_ID

        host = os.getenv("LLAMA_HOST", "localhost")
        port = int(os.getenv("LLAMA_PORT", 8080))
        client = LlamaStackClient(base_url=f"http://{host}:{port}", http_client=http_client)

        available_models = [
            model.identifier
            for model in client.models.list()
            if model.model_type == "llm" and "guard" not in model.identifier
        ]

        if MODEL_ID not in available_models:
            raise RuntimeError(f"Model `{MODEL_ID}` not found in {available_models}")

        agent = Agent(
            client,
            model=MODEL_ID,
            instructions="You are a helpful assistant. Use tools when necessary.",
            sampling_params={
                "strategy": {"type": "top_p", "temperature": 1.0, "top_p": 0.9},
            },
            #tools=[],
            tools=[calculator, "builtin::websearch"],
        )

        session_id = agent.create_session("openai-compatible-session")
        AGENT_INSTANCE = agent
        AGENT_SESSION_ID = session_id
        logger.info(f"Initialized agent with model {MODEL_ID} and session {session_id}")

    @app.on_event("startup")
    def on_startup():
        initialize_agent()

    @app.get("/v1/models", response_model=ModelList)
    def get_models():
        return ModelList(
            object="list",
            data=[ModelInfo(id=alias) for alias in MODEL_ALIAS_MAP.keys()]
        )

    @app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
    def chat_completions(request: ChatCompletionRequest, raw_request: Request):
        with tracer.start_as_current_span("agent.chat_completions") as span:
            # https://opentelemetry.io/docs/specs/semconv/attributes-registry/http/
            for header_name, header_value in raw_request.headers.items():
                span.set_attribute(f"http.request.header.{header_name}", header_value)

            internal_model = MODEL_ALIAS_MAP.get(request.model)
            if not internal_model:
                raise HTTPException(status_code=404, detail="Model not found")

            if internal_model != MODEL_ID or not AGENT_INSTANCE or not AGENT_SESSION_ID:
                raise HTTPException(status_code=500, detail="Agent not initialized")

            try:
                logger.info(f"All incoming messages: {[msg.dict() for msg in request.messages]}")
                agent_messages = [
                    {"role": "user", "content": msg.content}
                    for msg in request.messages
                    if msg.role == "user"
                ]

                if not agent_messages:
                    raise HTTPException(status_code=400, detail="No user message found.")
                # Trace the agent's create_turn method
                with tracer.start_as_current_span("agent.create_turn"):
                    response = AGENT_INSTANCE.create_turn(
                        messages=agent_messages,
                        session_id=AGENT_SESSION_ID,
                        stream=False,
                    )

                content = ""
                if hasattr(response, "output_message"):
                    content = response.output_message.content

                if not content:
                    raise ValueError("No response from assistant")

            except Exception as e:
                logger.exception("Chat completion error")
                raise HTTPException(status_code=500, detail="Chat agent error")

            return ChatCompletionResponse(
                id="chatcmpl-1234",
                object="chat.completion",
                created=int(time.time()),
                model=MODEL_ID,
                choices=[
                    ChatCompletionChoice(
                        index=0,
                        message=ChatMessage(role="assistant", content=content.strip()),
                        finish_reason="stop"
                    )
                ]
            )

    FastAPIInstrumentor.instrument_app(app)
kind: ConfigMap
metadata:
  name: llamastack-agent
  namespace: llm
